
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/general/plot_12_group_glm.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_general_plot_12_group_glm.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_general_plot_12_group_glm.py:


.. _tut-fnirs-group:

Group Level GLM Analysis
========================

This is an example of a group level GLM based
functional near-infrared spectroscopy (fNIRS)
analysis in MNE-NIRS.

.. sidebar:: Relevant literature

   Luke, Robert, et al.
   "Analysis methods for measuring passive auditory fNIRS responses generated
   by a block-design paradigm." Neurophotonics 8.2 (2021):
   `025008 <https://www.spiedigitallibrary.org/journals/neurophotonics/volume-8/issue-2/025008/Analysis-methods-for-measuring-passive-auditory-fNIRS-responses-generated-by/10.1117/1.NPh.8.2.025008.short>`_.

   Santosa, H., Zhai, X., Fishburn, F., & Huppert, T. (2018).
   The NIRS brain AnalyzIR toolbox. Algorithms, 11(5), 73.

   Gorgolewski, Krzysztof J., et al.
   "The brain imaging data structure, a format for organizing and describing
   outputs of neuroimaging experiments." Scientific data 3.1 (2016): 1-9.

Individual level analysis of this data is described in the
:ref:`MNE fNIRS waveform tutorial <mne:tut-fnirs-processing>`
and the
:ref:`MNE-NIRS fNIRS GLM tutorial <tut-fnirs-hrf>`
So this example will skim over the individual level details
and focus on the group level aspect of analysis.
Here we describe how to process multiple measurements
and summarise  group level effects both as summary statistics and visually.

The data used in this example is available
`at this location <https://github.com/rob-luke/BIDS-NIRS-Tapping>`_.
It is a finger tapping example and is briefly described below.
The dataset contains 5 participants.
The example dataset is in
`BIDS <https://bids.neuroimaging.io>`_
format and therefore already contains
information about triggers, condition names, etc.

.. note::

   This tutorial uses data in the BIDS format.
   The BIDS specification for NIRS data is still under development. See:
   `fNIRS BIDS proposal <https://github.com/bids-standard/bids-specification/pull/802>`_.
   As such, to run this tutorial you must use the fNIRS development branch of MNE-BIDS.

   To install the fNIRS development branch of MNE-BIDS run:
   `pip install -U https://codeload.github.com/rob-luke/mne-bids/zip/nirs`.

   MNE-Python. allows you to process fNIRS data that is not in BIDS format too.
   Simply modify the ``read_raw_`` function to match your data type.
   See :ref:`data importing tutorial <tut-importing-fnirs-data>` to learn how
   to use your data with MNE-Python.

.. note::

   Optodes were placed over the motor cortex using the standard NIRX motor
   montage, but with 8 short channels added (see their web page for details).
   To view the sensor locations run
   `raw_intensity.plot_sensors()`.
   A sound was presented to indicate which hand the participant should tap.
   Participants tapped their thumb to their fingers for 5s.
   Conditions were presented in a random order with a randomised inter
   stimulus interval.

.. contents:: Page contents
   :local:
   :depth: 2

.. GENERATED FROM PYTHON SOURCE LINES 73-111

.. code-block:: default

    # sphinx_gallery_thumbnail_number = 2

    # Authors: Robert Luke <mail@robertluke.net>
    #
    # License: BSD (3-clause)


    # Import common libraries
    import numpy as np
    import pandas as pd

    # Import MNE processing
    from mne.preprocessing.nirs import optical_density, beer_lambert_law

    # Import MNE-NIRS processing
    from mne_nirs.statistics import run_glm
    from mne_nirs.experimental_design import make_first_level_design_matrix
    from mne_nirs.statistics import statsmodels_to_results
    from mne_nirs.channels import get_short_channels, get_long_channels
    from mne_nirs.channels import picks_pair_to_idx
    from mne_nirs.visualisation import plot_glm_group_topo
    from mne_nirs.datasets import fnirs_motor_group
    from mne_nirs.visualisation import plot_glm_surface_projection
    from mne_nirs.io.fold import fold_channel_specificity

    # Import MNE-BIDS processing
    from mne_bids import BIDSPath, read_raw_bids, get_entity_vals

    # Import StatsModels
    import statsmodels.formula.api as smf

    # Import Plotting Library
    import matplotlib.pyplot as plt
    import matplotlib as mpl
    from lets_plot import *
    LetsPlot.setup_html()









.. GENERATED FROM PYTHON SOURCE LINES 112-126

Set up directories
------------------
.. sidebar:: Requires MNE-BIDS fNIRS branch

   This section of code requires the MNE-BIDS fNIRS branch.
   See instructions at the top of the page on how to install.
   Alternatively, if your data is not in BIDS format,
   skip to the next section.

First we will define where the raw data is stored. We will analyse a
BIDS dataset. This ensures we have all the metadata we require
without manually specifying the trigger names etc.
We first define where the root directory of our dataset is.
In this example we use the example dataset ``fnirs_motor_group``.

.. GENERATED FROM PYTHON SOURCE LINES 126-131

.. code-block:: default


    root = fnirs_motor_group.data_path()
    print(root)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/mne_data/fNIRS-motor-group




.. GENERATED FROM PYTHON SOURCE LINES 132-136

And as we are using MNE-BIDS we can create a BIDSPath object.
This class helps to handle all the path wrangling.
We inform the software that we are analysing nirs data that is saved in
the snirf format.

.. GENERATED FROM PYTHON SOURCE LINES 136-142

.. code-block:: default


    dataset = BIDSPath(root=root, task="tapping",
                       datatype="nirs", suffix="nirs", extension=".snirf")

    print(dataset.directory)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/runner/mne_data/fNIRS-motor-group/nirs




.. GENERATED FROM PYTHON SOURCE LINES 143-144

For example we can automatically query the subjects, tasks, and sessions.

.. GENERATED FROM PYTHON SOURCE LINES 144-149

.. code-block:: default


    subjects = get_entity_vals(root, 'subject')
    print(subjects)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    ['01', '02', '03', '04', '05']




.. GENERATED FROM PYTHON SOURCE LINES 150-183

Define individual analysis
--------------------------

.. sidebar:: Individual analysis procedures

   :ref:`Waveform individual analysis <tut-fnirs-processing>`

   :ref:`GLM individual analysis <tut-fnirs-hrf>`

First we define the analysis that will be applied to each file.
This is a GLM analysis as described in the
:ref:`individual GLM tutorial <tut-fnirs-hrf>`,
so this example will skim over the individual level details.

The analysis extracts a response estimate for each channel,
each region of interest, and computes a contrast between left and right
finger tapping.
We return the raw object and data frames for the computed results.
Information about channels, triggers and their meanings are stored in the
BIDS structure and are automatically obtained when importing the data.

Here we also resample to a 0.3 Hz sample rate just to speed up the example
and use less memory, resampling to 0.6 Hz is a better choice for full
analyses.

.. note::

   The nilearn library does not allow backslash characters in the condition
   name. So we must replace the backslash with an underscore to ensure the
   GLM computation is successful. Hopefully future versions of MNE-NIRS will
   automatically handle these characters, see https://github.com/mne-tools/mne-nirs/issues/420
   for more information. In the meantime use the following code to replace the
   illegal characters.

.. GENERATED FROM PYTHON SOURCE LINES 183-249

.. code-block:: default



    def individual_analysis(bids_path, ID):

        raw_intensity = read_raw_bids(bids_path=bids_path, verbose=False)
        # sanitize event names
        raw_intensity.annotations.description[:] = [
            d.replace('/', '_') for d in raw_intensity.annotations.description]

        # Convert signal to haemoglobin and resample
        raw_od = optical_density(raw_intensity)
        raw_haemo = beer_lambert_law(raw_od, ppf=0.1)
        raw_haemo.resample(0.3)

        # Cut out just the short channels for creating a GLM repressor
        sht_chans = get_short_channels(raw_haemo)
        raw_haemo = get_long_channels(raw_haemo)

        # Create a design matrix
        design_matrix = make_first_level_design_matrix(raw_haemo, stim_dur=5.0)

        # Append short channels mean to design matrix
        design_matrix["ShortHbO"] = np.mean(sht_chans.copy().pick(picks="hbo").get_data(), axis=0)
        design_matrix["ShortHbR"] = np.mean(sht_chans.copy().pick(picks="hbr").get_data(), axis=0)

        # Run GLM
        glm_est = run_glm(raw_haemo, design_matrix)

        # Define channels in each region of interest
        # List the channel pairs manually
        left = [[4, 3], [1, 3], [3, 3], [1, 2], [2, 3], [1, 1]]
        right = [[8, 7], [5, 7], [7, 7], [5, 6], [6, 7], [5, 5]]
        # Then generate the correct indices for each pair
        groups = dict(
            Left_Hemisphere=picks_pair_to_idx(raw_haemo, left, on_missing='ignore'),
            Right_Hemisphere=picks_pair_to_idx(raw_haemo, right, on_missing='ignore'))

        # Extract channel metrics
        cha = glm_est.to_dataframe()

        # Compute region of interest results from channel data
        roi = glm_est.to_dataframe_region_of_interest(groups,
                                                      design_matrix.columns,
                                                      demographic_info=True)

        # Define left vs right tapping contrast
        contrast_matrix = np.eye(design_matrix.shape[1])
        basic_conts = dict([(column, contrast_matrix[i])
                            for i, column in enumerate(design_matrix.columns)])
        contrast_LvR = basic_conts['Tapping_Left'] - basic_conts['Tapping_Right']

        # Compute defined contrast
        contrast = glm_est.compute_contrast(contrast_LvR)
        con = contrast.to_dataframe()

        # Add the participant ID to the dataframes
        roi["ID"] = cha["ID"] = con["ID"] = ID

        # Convert to uM for nicer plotting below.
        cha["theta"] = [t * 1.e6 for t in cha["theta"]]
        roi["theta"] = [t * 1.e6 for t in roi["theta"]]
        con["effect"] = [t * 1.e6 for t in con["effect"]]

        return raw_haemo, roi, cha, con









.. GENERATED FROM PYTHON SOURCE LINES 250-257

Run analysis on all participants
--------------------------------

Next we loop through the five measurements and run the individual analysis
on each. We append the individual results in to a large dataframe that
will contain the results from all measurements. We create a group dataframe
for the region of interest, channel level, and contrast results.

.. GENERATED FROM PYTHON SOURCE LINES 257-276

.. code-block:: default


    df_roi = pd.DataFrame()  # To store region of interest results
    df_cha = pd.DataFrame()  # To store channel level results
    df_con = pd.DataFrame()  # To store channel level contrast results

    for sub in subjects:  # Loop from first to fifth subject

        # Create path to file based on experiment info
        bids_path = dataset.update(subject=sub)

        # Analyse data and return both ROI and channel results
        raw_haemo, roi, channel, con = individual_analysis(bids_path, sub)

        # Append individual results to all participants
        df_roi = pd.concat([df_roi, roi], ignore_index=True)
        df_cha = pd.concat([df_cha, channel], ignore_index=True)
        df_con = pd.concat([df_con, con], ignore_index=True)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Reading 0 ... 23238  =      0.000 ...  2974.464 secs...
    Reading 0 ... 18877  =      0.000 ...  2416.256 secs...
    Reading 0 ... 18874  =      0.000 ...  2415.872 secs...
    Reading 0 ... 23120  =      0.000 ...  2959.360 secs...
    Reading 0 ... 23006  =      0.000 ...  2944.768 secs...




.. GENERATED FROM PYTHON SOURCE LINES 277-286

Visualise Individual results
----------------------------

First we visualise the results from each individual to ensure the
data values look reasonable.
Here we see that we have data from five participants, we plot just the HbO
values and observe they are in the expect range.
We can already see that the control condition is always near zero,
and that the responses look to be contralateral to the tapping hand.

.. GENERATED FROM PYTHON SOURCE LINES 286-297

.. code-block:: default


    grp_results = df_roi.query("Condition in ['Control', 'Tapping_Left', 'Tapping_Right']")
    grp_results = grp_results.query("Chroma in ['hbo']")

    ggplot(grp_results, aes(x='Condition', y='theta', color='ROI', shape='ROI')) \
        + geom_hline(y_intercept=0, linetype="dashed", size=1) \
        + geom_point(size=5) \
        + facet_grid('ID') \
        + ggsize(900, 350)







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <html lang="en">
       <head>
           <script type="text/javascript" data-lets-plot-script="library" src="https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.2.1/js-package/distr/lets-plot.min.js"></script>
       </head>
       <body>
              <div id="L5cCmf"></div>
       <script type="text/javascript" data-lets-plot-script="plot">
           var plotSpec={
    "data":{
    "ROI":["Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere"],
    "Condition":["Control","Control","Tapping_Left","Tapping_Left","Tapping_Right","Tapping_Right","Control","Control","Tapping_Left","Tapping_Left","Tapping_Right","Tapping_Right","Control","Control","Tapping_Left","Tapping_Left","Tapping_Right","Tapping_Right","Control","Control","Tapping_Left","Tapping_Left","Tapping_Right","Tapping_Right","Control","Control","Tapping_Left","Tapping_Left","Tapping_Right","Tapping_Right"],
    "theta":[0.3665201796697473,0.27670524778352773,2.253306244287182,5.653439098291439,7.092229283099195,3.184357597259925,-0.9143790325342311,-1.8651492092869275,3.148509079777711,5.773968178095921,7.817526976164081,1.861371274363865,-0.8896275974260027,-2.1196440112667676,0.9805964990151607,5.06297102691765,3.4573458561590313,2.9490687218306815,-0.03231701254771904,-0.20974555678101225,-3.1943037212375693,10.659305305843992,4.992717421875512,-0.6397740924533993,0.041583220360193705,-0.4782130897061909,6.6621849196694445,23.34021027816171,22.98954259085975,11.577755280391834],
    "ID":["01","01","01","01","01","01","02","02","02","02","02","02","03","03","03","03","03","03","04","04","04","04","04","04","05","05","05","05","05","05"]
    },
    "mapping":{
    "x":"Condition",
    "y":"theta",
    "color":"ROI",
    "shape":"ROI"
    },
    "data_meta":{
    },
    "facet":{
    "name":"grid",
    "x":"ID",
    "x_order":1,
    "y_order":1
    },
    "ggsize":{
    "width":900,
    "height":350
    },
    "kind":"plot",
    "scales":[],
    "layers":[{
    "geom":"hline",
    "mapping":{
    },
    "data_meta":{
    },
    "y_intercept":0,
    "linetype":"dashed",
    "size":1,
    "data":{
    }
    },{
    "geom":"point",
    "mapping":{
    },
    "data_meta":{
    },
    "size":5,
    "data":{
    }
    }]
    };
           var plotContainer = document.getElementById("L5cCmf");
           LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);
       </script>
       </body>
    </html>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 298-329

Compute group level results
---------------------------

.. sidebar:: Relevant literature

   For an introduction to mixed effects analysis see:
   Winter, Bodo. "A very basic tutorial for performing linear mixed effects
   analyses." arXiv preprint arXiv:1308.5499 (2013).

   For a summary of linear mixed models in python
   and the relation to lmer see:
   :ref:`statsmodels docs <statsmodels:mixedlmmod>`

   For a summary of these models in the context of fNIRS see section 3.5 of:
   Santosa, H., Zhai, X., Fishburn, F., & Huppert, T. (2018).
   The NIRS brain AnalyzIR toolbox. Algorithms, 11(5), 73.

Next we use a linear mixed effects model to examine the
relation between conditions and our response estimate (theta).
Combinations of 3 fixed effects will be evaluated, ROI (left vs right),
condition (control, tapping/left, tapping/right), and chromophore (HbO, HbR).
With a random effect of subject.
Alternatively, you could export the group dataframe (`df_roi.to_csv()`) and
analyse in your favorite stats program.

We do not explore the modeling procedure in depth here as topics
such model selection and examining residuals are beyond the scope of
this example (see relevant literature).
Alternatively, we could use a robust linear
model by using the code
`roi_model = rlm('theta ~ -1 + ROI:Condition:Chroma', grp_results).fit()`.

.. GENERATED FROM PYTHON SOURCE LINES 329-337

.. code-block:: default


    grp_results = df_roi.query("Condition in ['Control','Tapping_Left', 'Tapping_Right']")

    roi_model = smf.mixedlm("theta ~ -1 + ROI:Condition:Chroma",
                            grp_results, groups=grp_results["ID"]).fit(method='nm')
    roi_model.summary()






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.9.10/x64/lib/python3.9/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.
      warnings.warn(msg, ConvergenceWarning)


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <table class="simpletable">
    <tr>
           <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>   <td>theta</td>  
    </tr>
    <tr>
      <td>No. Observations:</td>   <td>60</td>          <td>Method:</td>         <td>REML</td>   
    </tr>
    <tr>
         <td>No. Groups:</td>       <td>5</td>          <td>Scale:</td>         <td>15.9978</td> 
    </tr>
    <tr>
      <td>Min. group size:</td>    <td>12</td>      <td>Log-Likelihood:</td>   <td>-144.3046</td>
    </tr>
    <tr>
      <td>Max. group size:</td>    <td>12</td>        <td>Converged:</td>         <td>Yes</td>   
    </tr>
    <tr>
      <td>Mean group size:</td>   <td>12.0</td>            <td></td>               <td></td>     
    </tr>
    </table>
    <table class="simpletable">
    <tr>
                                   <td></td>                               <th>Coef.</th> <th>Std.Err.</th>    <th>z</th>   <th>P>|z|</th> <th>[0.025</th> <th>0.975]</th>
    </tr>
    <tr>
      <th>ROI[Left_Hemisphere]:Condition[Control]:Chroma[hbo]</th>        <td>-0.286</td>   <td>1.789</td>  <td>-0.160</td> <td>0.873</td> <td>-3.791</td>  <td>3.220</td>
    </tr>
    <tr>
      <th>ROI[Right_Hemisphere]:Condition[Control]:Chroma[hbo]</th>       <td>-0.879</td>   <td>1.789</td>  <td>-0.492</td> <td>0.623</td> <td>-4.385</td>  <td>2.627</td>
    </tr>
    <tr>
      <th>ROI[Left_Hemisphere]:Condition[Tapping_Left]:Chroma[hbo]</th>    <td>1.970</td>   <td>1.789</td>   <td>1.101</td> <td>0.271</td> <td>-1.536</td>  <td>5.476</td>
    </tr>
    <tr>
      <th>ROI[Right_Hemisphere]:Condition[Tapping_Left]:Chroma[hbo]</th>  <td>10.098</td>   <td>1.789</td>   <td>5.645</td> <td>0.000</td>  <td>6.592</td> <td>13.604</td>
    </tr>
    <tr>
      <th>ROI[Left_Hemisphere]:Condition[Tapping_Right]:Chroma[hbo]</th>   <td>9.270</td>   <td>1.789</td>   <td>5.182</td> <td>0.000</td>  <td>5.764</td> <td>12.776</td>
    </tr>
    <tr>
      <th>ROI[Right_Hemisphere]:Condition[Tapping_Right]:Chroma[hbo]</th>  <td>3.787</td>   <td>1.789</td>   <td>2.117</td> <td>0.034</td>  <td>0.281</td>  <td>7.292</td>
    </tr>
    <tr>
      <th>ROI[Left_Hemisphere]:Condition[Control]:Chroma[hbr]</th>         <td>0.269</td>   <td>1.789</td>   <td>0.150</td> <td>0.881</td> <td>-3.237</td>  <td>3.775</td>
    </tr>
    <tr>
      <th>ROI[Right_Hemisphere]:Condition[Control]:Chroma[hbr]</th>        <td>0.140</td>   <td>1.789</td>   <td>0.078</td> <td>0.937</td> <td>-3.365</td>  <td>3.646</td>
    </tr>
    <tr>
      <th>ROI[Left_Hemisphere]:Condition[Tapping_Left]:Chroma[hbr]</th>   <td>-2.010</td>   <td>1.789</td>  <td>-1.124</td> <td>0.261</td> <td>-5.516</td>  <td>1.496</td>
    </tr>
    <tr>
      <th>ROI[Right_Hemisphere]:Condition[Tapping_Left]:Chroma[hbr]</th>  <td>-4.428</td>   <td>1.789</td>  <td>-2.476</td> <td>0.013</td> <td>-7.934</td> <td>-0.923</td>
    </tr>
    <tr>
      <th>ROI[Left_Hemisphere]:Condition[Tapping_Right]:Chroma[hbr]</th>  <td>-4.130</td>   <td>1.789</td>  <td>-2.309</td> <td>0.021</td> <td>-7.636</td> <td>-0.624</td>
    </tr>
    <tr>
      <th>ROI[Right_Hemisphere]:Condition[Tapping_Right]:Chroma[hbr]</th> <td>-1.956</td>   <td>1.789</td>  <td>-1.094</td> <td>0.274</td> <td>-5.462</td>  <td>1.550</td>
    </tr>
    <tr>
      <th>Group Var</th>                                                   <td>0.000</td>   <td>0.275</td>     <td></td>      <td></td>       <td></td>       <td></td>   
    </tr>
    </table>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 338-359

Second level analysis with covariates
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

.. sidebar:: Relevant literature

   For a detailed discussion about covariates in fNIRS analysis see
   the seminar by Dr. Jessica Gemignani
   (`youtube <https://www.youtube.com/watch?feature=emb_logo&v=3E28sT1JI14>`_).

It is simple to extend these models to include covariates.
This dataset is small, so including additional factors may not be
appropriate. However, for instructional purpose, we will include a
covariate of gender. There are 3 females and 2 males in this dataset.
Also, for instructional purpose, we modify the model
above to only explore the difference between the two tapping conditions in
the hbo signal in the right hemisphere.

From the model result we observe that hbo responses in the right hemisphere
are smaller when the right hand was used (as expected for these
contralaterally dominant responses) and there is no significant
effect of gender.

.. GENERATED FROM PYTHON SOURCE LINES 359-368

.. code-block:: default


    grp_results = df_roi.query("Condition in ['Tapping_Left', 'Tapping_Right']")
    grp_results = grp_results.query("Chroma in ['hbo']")
    grp_results = grp_results.query("ROI in ['Right_Hemisphere']")

    roi_model = smf.mixedlm("theta ~ Condition + Sex",
                            grp_results, groups=grp_results["ID"]).fit(method='nm')
    roi_model.summary()






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <table class="simpletable">
    <tr>
           <td>Model:</td>       <td>MixedLM</td> <td>Dependent Variable:</td>   <td>theta</td> 
    </tr>
    <tr>
      <td>No. Observations:</td>   <td>10</td>          <td>Method:</td>         <td>REML</td>  
    </tr>
    <tr>
         <td>No. Groups:</td>       <td>5</td>          <td>Scale:</td>         <td>11.5908</td>
    </tr>
    <tr>
      <td>Min. group size:</td>     <td>2</td>      <td>Log-Likelihood:</td>   <td>-22.9508</td>
    </tr>
    <tr>
      <td>Max. group size:</td>     <td>2</td>        <td>Converged:</td>         <td>Yes</td>  
    </tr>
    <tr>
      <td>Mean group size:</td>    <td>2.0</td>            <td></td>               <td></td>    
    </tr>
    </table>
    <table class="simpletable">
    <tr>
                   <td></td>               <th>Coef.</th> <th>Std.Err.</th>    <th>z</th>   <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th>
    </tr>
    <tr>
      <th>Intercept</th>                   <td>7.434</td>   <td>3.271</td>   <td>2.273</td> <td>0.023</td>  <td>1.023</td>  <td>13.844</td>
    </tr>
    <tr>
      <th>Condition[T.Tapping_Right]</th> <td>-6.311</td>   <td>2.153</td>  <td>-2.931</td> <td>0.003</td> <td>-10.532</td> <td>-2.091</td>
    </tr>
    <tr>
      <th>Sex[T.male]</th>                 <td>6.661</td>   <td>4.883</td>   <td>1.364</td> <td>0.173</td> <td>-2.910</td>  <td>16.232</td>
    </tr>
    <tr>
      <th>Group Var</th>                  <td>22.820</td>   <td>9.079</td>     <td></td>      <td></td>       <td></td>        <td></td>   
    </tr>
    </table>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 369-380

Visualise group results
-----------------------

Now we can summarise the output of the second level model.
This figure shows that the control condition has small responses that
are not significantly different to zero for both HbO
and HbR in both hemispheres.
Whereas clear significant responses are show for the two tapping conditions.
We also observe the the tapping response is
larger in the contralateral hemisphere.
Filled symbols represent HbO, unfilled symbols represent HbR.

.. GENERATED FROM PYTHON SOURCE LINES 380-400

.. code-block:: default


    # Regenerate the results from the original group model above
    grp_results = df_roi.query("Condition in ['Control','Tapping_Left', 'Tapping_Right']")
    roi_model = smf.mixedlm("theta ~ -1 + ROI:Condition:Chroma",
                            grp_results, groups=grp_results["ID"]).fit(method='nm')

    df = statsmodels_to_results(roi_model)

    ggplot(df.query("Chroma == 'hbo'"),
           aes(x='Condition', y='Coef.', color='Significant', shape='ROI')) \
        + geom_hline(y_intercept=0, linetype="dashed", size=1) \
        + geom_point(size=5) \
        + scale_shape_manual(values=[16, 17]) \
        + ggsize(900, 300) \
        + geom_point(data=df.query("Chroma == 'hbr'")
                     .query("ROI == 'Left_Hemisphere'"), size=5, shape=1) \
        + geom_point(data=df.query("Chroma == 'hbr'")
                     .query("ROI == 'Right_Hemisphere'"), size=5, shape=2)






.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.9.10/x64/lib/python3.9/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.
      warnings.warn(msg, ConvergenceWarning)


.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <html lang="en">
       <head>
           <script type="text/javascript" data-lets-plot-script="library" src="https://cdn.jsdelivr.net/gh/JetBrains/lets-plot@v2.2.1/js-package/distr/lets-plot.min.js"></script>
       </head>
       <body>
              <div id="KWYajs"></div>
       <script type="text/javascript" data-lets-plot-script="plot">
           var plotSpec={
    "data":{
    "Coef.":[-0.2856440484956024,-0.8792093238514743,1.970058604302385,10.09797877746214,9.269872425631515,3.7865557562785805],
    "ROI":["Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere","Left_Hemisphere","Right_Hemisphere"],
    "Condition":["Control","Control","Tapping_Left","Tapping_Left","Tapping_Right","Tapping_Right"],
    "Significant":[false,false,false,true,true,true]
    },
    "mapping":{
    "x":"Condition",
    "y":"Coef.",
    "color":"Significant",
    "shape":"ROI"
    },
    "data_meta":{
    },
    "ggsize":{
    "width":900,
    "height":300
    },
    "kind":"plot",
    "scales":[{
    "aesthetic":"shape",
    "values":[16,17]
    }],
    "layers":[{
    "geom":"hline",
    "mapping":{
    },
    "data_meta":{
    },
    "y_intercept":0,
    "linetype":"dashed",
    "size":1,
    "data":{
    }
    },{
    "geom":"point",
    "mapping":{
    },
    "data_meta":{
    },
    "size":5,
    "data":{
    }
    },{
    "geom":"point",
    "data":{
    "Coef.":[0.2686572808024586,-2.0102302437499233,-4.130176130165142],
    "Condition":["Control","Tapping_Left","Tapping_Right"],
    "Significant":[false,false,true]
    },
    "mapping":{
    },
    "data_meta":{
    },
    "size":5,
    "shape":1
    },{
    "geom":"point",
    "data":{
    "Coef.":[0.14039044097632833,-4.428427669198316,-1.9560838108155179],
    "Condition":["Control","Tapping_Left","Tapping_Right"],
    "Significant":[false,true,false]
    },
    "mapping":{
    },
    "data_meta":{
    },
    "size":5,
    "shape":2
    }]
    };
           var plotContainer = document.getElementById("KWYajs");
           LetsPlot.buildPlotFromProcessedSpecs(plotSpec, -1, -1, plotContainer);
       </script>
       </body>
    </html>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 401-410

Group topographic visualisation
-------------------------------

We can also view the topographic representation of the data
(rather than the ROI summary above).
Here we just plot the oxyhaemoglobin for the two tapping conditions.
First we compute the mixed effects model for each channel (rather
than region of interest as above).
Then we pass these results to the topomap function.

.. GENERATED FROM PYTHON SOURCE LINES 410-454

.. code-block:: default


    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 10),
                             gridspec_kw=dict(width_ratios=[1, 1]))

    # Cut down the dataframe just to the conditions we are interested in
    ch_summary = df_cha.query("Condition in ['Tapping_Left', 'Tapping_Right']")
    ch_summary = ch_summary.query("Chroma in ['hbo']")

    # Run group level model and convert to dataframe
    ch_model = smf.mixedlm("theta ~ -1 + ch_name:Chroma:Condition",
                           ch_summary, groups=ch_summary["ID"]).fit(method='nm')
    ch_model_df = statsmodels_to_results(ch_model)

    # Plot the two conditions
    plot_glm_group_topo(raw_haemo.copy().pick(picks="hbo"),
                        ch_model_df.query("Condition in ['Tapping_Left']"),
                        colorbar=False, axes=axes[0, 0],
                        vmin=0, vmax=20, cmap=mpl.cm.Oranges)

    plot_glm_group_topo(raw_haemo.copy().pick(picks="hbo"),
                        ch_model_df.query("Condition in ['Tapping_Right']"),
                        colorbar=True, axes=axes[0, 1],
                        vmin=0, vmax=20, cmap=mpl.cm.Oranges)

    # Cut down the dataframe just to the conditions we are interested in
    ch_summary = df_cha.query("Condition in ['Tapping_Left', 'Tapping_Right']")
    ch_summary = ch_summary.query("Chroma in ['hbr']")

    # Run group level model and convert to dataframe
    ch_model = smf.mixedlm("theta ~ -1 + ch_name:Chroma:Condition",
                           ch_summary, groups=ch_summary["ID"]).fit(method='nm')
    ch_model_df = statsmodels_to_results(ch_model)

    # Plot the two conditions
    plot_glm_group_topo(raw_haemo.copy().pick(picks="hbr"),
                        ch_model_df.query("Condition in ['Tapping_Left']"),
                        colorbar=False, axes=axes[1, 0],
                        vmin=-10, vmax=0, cmap=mpl.cm.Blues_r)
    plot_glm_group_topo(raw_haemo.copy().pick(picks="hbr"),
                        ch_model_df.query("Condition in ['Tapping_Right']"),
                        colorbar=True, axes=axes[1, 1],
                        vmin=-10, vmax=0, cmap=mpl.cm.Blues_r)





.. image-sg:: /auto_examples/general/images/sphx_glr_plot_12_group_glm_001.png
   :alt: Tapping_Left, Tapping_Right, Tapping_Left, Tapping_Right
   :srcset: /auto_examples/general/images/sphx_glr_plot_12_group_glm_001.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    <AxesSubplot:title={'center':'Tapping_Right'}>



.. GENERATED FROM PYTHON SOURCE LINES 455-461

Contrasts
---------

Finally we can examine the difference between the left and right hand
tapping conditions by viewing the contrast results
in a topographic representation.

.. GENERATED FROM PYTHON SOURCE LINES 461-476

.. code-block:: default


    fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(6, 6))
    con_summary = df_con.query("Chroma in ['hbo']")

    # Run group level model and convert to dataframe
    con_model = smf.mixedlm("effect ~ -1 + ch_name:Chroma",
                            con_summary, groups=con_summary["ID"]).fit(method='nm')
    con_model_df = statsmodels_to_results(con_model,
                                          order=raw_haemo.copy().pick(
                                              picks="hbo").ch_names)

    plot_glm_group_topo(raw_haemo.copy().pick(picks="hbo"),
                        con_model_df, colorbar=True, axes=axes)





.. image-sg:: /auto_examples/general/images/sphx_glr_plot_12_group_glm_002.png
   :alt: Contrast
   :srcset: /auto_examples/general/images/sphx_glr_plot_12_group_glm_002.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /opt/hostedtoolcache/Python/3.9.10/x64/lib/python3.9/site-packages/statsmodels/regression/mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.
      warnings.warn(msg, ConvergenceWarning)
    /opt/hostedtoolcache/Python/3.9.10/x64/lib/python3.9/site-packages/statsmodels/regression/mixed_linear_model.py:2261: ConvergenceWarning: The Hessian matrix at the estimated parameter values is not positive definite.
      warnings.warn(msg, ConvergenceWarning)

    <AxesSubplot:title={'center':'Contrast'}>



.. GENERATED FROM PYTHON SOURCE LINES 477-480

Or we can view only the left hemisphere for the contrast.
And set all channels that dont have a significant response to zero.


.. GENERATED FROM PYTHON SOURCE LINES 481-486

.. code-block:: default


    plot_glm_group_topo(raw_haemo.copy().pick(picks="hbo").pick(picks=range(10)),
                        con_model_df, colorbar=True, threshold=True)





.. image-sg:: /auto_examples/general/images/sphx_glr_plot_12_group_glm_003.png
   :alt: Contrast
   :srcset: /auto_examples/general/images/sphx_glr_plot_12_group_glm_003.png
   :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Reducing GLM results to match MNE data

    <AxesSubplot:title={'center':'Contrast'}>



.. GENERATED FROM PYTHON SOURCE LINES 487-501

Cortical Surface Projections
----------------------------

The topographic plots above can sometimes be difficult to interpret with
respect to the underlying cortical locations. It is also possible to present
the data by projecting the channel level GLM values to the nearest cortical
surface. This can make it easier to understand the spatial aspects of your
data. Note however, that this is not a complete forward model with photon
migration simulations.
In the figure below we project the group results from the two conditions
to the cortical surface, and also present the contrast results in the same
fashion.
As in the topo plots above you can see that the activity is predominately
contralateral to the side of finger tapping.

.. GENERATED FROM PYTHON SOURCE LINES 501-528

.. code-block:: default



    # Generate brain figure from data
    clim = dict(kind='value', pos_lims=(0, 8, 11))
    brain = plot_glm_surface_projection(raw_haemo.copy().pick("hbo"),
                                        con_model_df, clim=clim, view='dorsal',
                                        colorbar=True, size=(800, 700))
    brain.add_text(0.05, 0.95, "Left-Right", 'title', font_size=16, color='k')

    # Run model code as above
    clim = dict(kind='value', pos_lims=(0, 11.5, 17))
    for idx, cond in enumerate(['Tapping_Left', 'Tapping_Right']):

        # Run same model as explained in the sections above
        ch_summary = df_cha.query("Condition in [@cond]")
        ch_summary = ch_summary.query("Chroma in ['hbo']")
        ch_model = smf.mixedlm("theta ~ -1 + ch_name", ch_summary,
                               groups=ch_summary["ID"]).fit(method='nm')
        model_df = statsmodels_to_results(ch_model, order=raw_haemo.copy().pick("hbo").ch_names)

        # Generate brain figure from data
        brain = plot_glm_surface_projection(raw_haemo.copy().pick("hbo"),
                                            model_df, clim=clim, view='dorsal',
                                            colorbar=True, size=(800, 700))
        brain.add_text(0.05, 0.95, cond, 'title', font_size=16, color='k')





.. image-sg:: /auto_examples/general/images/sphx_glr_plot_12_group_glm_004.png
   :alt: plot 12 group glm
   :srcset: /auto_examples/general/images/sphx_glr_plot_12_group_glm_004.png
   :class: sphx-glr-single-img

.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /auto_examples/general/images/sphx_glr_plot_12_group_glm_005.png
          :alt: plot 12 group glm
          :srcset: /auto_examples/general/images/sphx_glr_plot_12_group_glm_005.png
          :class: sphx-glr-multi-img

    *

      .. image-sg:: /auto_examples/general/images/sphx_glr_plot_12_group_glm_006.png
          :alt: plot 12 group glm
          :srcset: /auto_examples/general/images/sphx_glr_plot_12_group_glm_006.png
          :class: sphx-glr-multi-img





.. GENERATED FROM PYTHON SOURCE LINES 529-534

Table of channel level results
------------------------------

Sometimes a reviewer wants a long table of results per channel.
This can be generated from the statistics dataframe.

.. GENERATED FROM PYTHON SOURCE LINES 534-552

.. code-block:: default


    ch_summary = df_cha.query("Condition in ['Tapping_Left', 'Tapping_Right']")
    ch_summary = ch_summary.query("Chroma in ['hbo']")

    # Run group level model and convert to dataframe
    ch_model = smf.mixedlm("theta ~ -1 + ch_name:Chroma:Condition",
                           ch_summary, groups=ch_summary["ID"]).fit(method='nm')

    # Here we can use the order argument to ensure the channel name order
    ch_model_df = statsmodels_to_results(ch_model,
                                         order=raw_haemo.copy().pick(
                                             picks="hbo").ch_names)
    # And make the table prettier
    ch_model_df.reset_index(drop=True, inplace=True)
    ch_model_df = ch_model_df.set_index(['ch_name', 'Condition'])
    ch_model_df







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th></th>
          <th>Coef.</th>
          <th>Std.Err.</th>
          <th>z</th>
          <th>P&gt;|z|</th>
          <th>[0.025</th>
          <th>0.975]</th>
          <th>Chroma</th>
          <th>Significant</th>
        </tr>
        <tr>
          <th>ch_name</th>
          <th>Condition</th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
          <th></th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th rowspan="2" valign="top">S1_D1 hbo</th>
          <th>Tapping_Left</th>
          <td>2.725038</td>
          <td>4.189391</td>
          <td>0.650462</td>
          <td>0.515394</td>
          <td>-5.486018</td>
          <td>10.936094</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>6.592228</td>
          <td>4.189391</td>
          <td>1.573553</td>
          <td>0.115591</td>
          <td>-1.618829</td>
          <td>14.803284</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S1_D2 hbo</th>
          <th>Tapping_Left</th>
          <td>0.747997</td>
          <td>4.189391</td>
          <td>0.178546</td>
          <td>0.858295</td>
          <td>-7.463059</td>
          <td>8.959054</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>7.154465</td>
          <td>4.189391</td>
          <td>1.707758</td>
          <td>0.087681</td>
          <td>-1.056591</td>
          <td>15.365521</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S1_D3 hbo</th>
          <th>Tapping_Left</th>
          <td>2.003291</td>
          <td>4.189391</td>
          <td>0.478182</td>
          <td>0.632521</td>
          <td>-6.207765</td>
          <td>10.214347</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>17.029723</td>
          <td>4.189391</td>
          <td>4.064963</td>
          <td>0.000048</td>
          <td>8.818667</td>
          <td>25.24078</td>
          <td>hbo</td>
          <td>True</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S2_D1 hbo</th>
          <th>Tapping_Left</th>
          <td>-0.030335</td>
          <td>4.189391</td>
          <td>-0.007241</td>
          <td>0.994223</td>
          <td>-8.241391</td>
          <td>8.180721</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>1.185932</td>
          <td>4.189391</td>
          <td>0.283080</td>
          <td>0.777116</td>
          <td>-7.025124</td>
          <td>9.396988</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S2_D3 hbo</th>
          <th>Tapping_Left</th>
          <td>1.009418</td>
          <td>4.189391</td>
          <td>0.240946</td>
          <td>0.809597</td>
          <td>-7.201638</td>
          <td>9.220475</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>6.895600</td>
          <td>4.189391</td>
          <td>1.645967</td>
          <td>0.099771</td>
          <td>-1.315456</td>
          <td>15.106657</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S2_D4 hbo</th>
          <th>Tapping_Left</th>
          <td>-1.272552</td>
          <td>4.189391</td>
          <td>-0.303756</td>
          <td>0.761314</td>
          <td>-9.483608</td>
          <td>6.938504</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>0.170163</td>
          <td>4.189391</td>
          <td>0.040618</td>
          <td>0.967601</td>
          <td>-8.040893</td>
          <td>8.381219</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S3_D2 hbo</th>
          <th>Tapping_Left</th>
          <td>3.601137</td>
          <td>4.189391</td>
          <td>0.859585</td>
          <td>0.390018</td>
          <td>-4.60992</td>
          <td>11.812193</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>9.612044</td>
          <td>4.189391</td>
          <td>2.294377</td>
          <td>0.021769</td>
          <td>1.400987</td>
          <td>17.8231</td>
          <td>hbo</td>
          <td>True</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S3_D3 hbo</th>
          <th>Tapping_Left</th>
          <td>3.058901</td>
          <td>4.189391</td>
          <td>0.730154</td>
          <td>0.465296</td>
          <td>-5.152155</td>
          <td>11.269957</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>13.372372</td>
          <td>4.189391</td>
          <td>3.191961</td>
          <td>0.001413</td>
          <td>5.161316</td>
          <td>21.583429</td>
          <td>hbo</td>
          <td>True</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S4_D3 hbo</th>
          <th>Tapping_Left</th>
          <td>2.828156</td>
          <td>4.189391</td>
          <td>0.675076</td>
          <td>0.499628</td>
          <td>-5.382901</td>
          <td>11.039212</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>9.813455</td>
          <td>4.189391</td>
          <td>2.342453</td>
          <td>0.019157</td>
          <td>1.602398</td>
          <td>18.024511</td>
          <td>hbo</td>
          <td>True</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S4_D4 hbo</th>
          <th>Tapping_Left</th>
          <td>5.163194</td>
          <td>4.189391</td>
          <td>1.232445</td>
          <td>0.217783</td>
          <td>-3.047862</td>
          <td>13.374251</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>7.303296</td>
          <td>4.189391</td>
          <td>1.743283</td>
          <td>0.081284</td>
          <td>-0.907761</td>
          <td>15.514352</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S5_D5 hbo</th>
          <th>Tapping_Left</th>
          <td>5.693955</td>
          <td>4.189391</td>
          <td>1.359137</td>
          <td>0.174103</td>
          <td>-2.517101</td>
          <td>13.905011</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>3.160458</td>
          <td>4.189391</td>
          <td>0.754395</td>
          <td>0.450612</td>
          <td>-5.050599</td>
          <td>11.371514</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S5_D6 hbo</th>
          <th>Tapping_Left</th>
          <td>2.682824</td>
          <td>4.189391</td>
          <td>0.640385</td>
          <td>0.521922</td>
          <td>-5.528233</td>
          <td>10.89388</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>1.507595</td>
          <td>4.189391</td>
          <td>0.359860</td>
          <td>0.718952</td>
          <td>-6.703461</td>
          <td>9.718652</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S5_D7 hbo</th>
          <th>Tapping_Left</th>
          <td>19.332204</td>
          <td>4.189391</td>
          <td>4.614561</td>
          <td>0.000004</td>
          <td>11.121148</td>
          <td>27.54326</td>
          <td>hbo</td>
          <td>True</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>4.468471</td>
          <td>4.189391</td>
          <td>1.066616</td>
          <td>0.286145</td>
          <td>-3.742586</td>
          <td>12.679527</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S6_D5 hbo</th>
          <th>Tapping_Left</th>
          <td>3.000136</td>
          <td>4.189391</td>
          <td>0.716127</td>
          <td>0.473913</td>
          <td>-5.21092</td>
          <td>11.211193</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>2.584979</td>
          <td>4.189391</td>
          <td>0.617030</td>
          <td>0.537215</td>
          <td>-5.626078</td>
          <td>10.796035</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S6_D7 hbo</th>
          <th>Tapping_Left</th>
          <td>12.589845</td>
          <td>4.189391</td>
          <td>3.005173</td>
          <td>0.002654</td>
          <td>4.378789</td>
          <td>20.800902</td>
          <td>hbo</td>
          <td>True</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>4.669816</td>
          <td>4.189391</td>
          <td>1.114677</td>
          <td>0.264989</td>
          <td>-3.54124</td>
          <td>12.880873</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S6_D8 hbo</th>
          <th>Tapping_Left</th>
          <td>10.559629</td>
          <td>4.189391</td>
          <td>2.520564</td>
          <td>0.011717</td>
          <td>2.348573</td>
          <td>18.770685</td>
          <td>hbo</td>
          <td>True</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>6.095097</td>
          <td>4.189391</td>
          <td>1.454889</td>
          <td>0.145700</td>
          <td>-2.115959</td>
          <td>14.306154</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S7_D6 hbo</th>
          <th>Tapping_Left</th>
          <td>8.613836</td>
          <td>4.189391</td>
          <td>2.056107</td>
          <td>0.039772</td>
          <td>0.40278</td>
          <td>16.824892</td>
          <td>hbo</td>
          <td>True</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>3.928645</td>
          <td>4.189391</td>
          <td>0.937760</td>
          <td>0.348368</td>
          <td>-4.282412</td>
          <td>12.139701</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S7_D7 hbo</th>
          <th>Tapping_Left</th>
          <td>10.974402</td>
          <td>4.189391</td>
          <td>2.619570</td>
          <td>0.008804</td>
          <td>2.763346</td>
          <td>19.185459</td>
          <td>hbo</td>
          <td>True</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>3.907378</td>
          <td>4.189391</td>
          <td>0.932684</td>
          <td>0.350983</td>
          <td>-4.303678</td>
          <td>12.118435</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S8_D7 hbo</th>
          <th>Tapping_Left</th>
          <td>16.481581</td>
          <td>4.189391</td>
          <td>3.934123</td>
          <td>0.000084</td>
          <td>8.270524</td>
          <td>24.692637</td>
          <td>hbo</td>
          <td>True</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>6.938884</td>
          <td>4.189391</td>
          <td>1.656299</td>
          <td>0.097661</td>
          <td>-1.272172</td>
          <td>15.14994</td>
          <td>hbo</td>
          <td>False</td>
        </tr>
        <tr>
          <th rowspan="2" valign="top">S8_D8 hbo</th>
          <th>Tapping_Left</th>
          <td>11.664569</td>
          <td>4.189391</td>
          <td>2.784311</td>
          <td>0.005364</td>
          <td>3.453512</td>
          <td>19.875625</td>
          <td>hbo</td>
          <td>True</td>
        </tr>
        <tr>
          <th>Tapping_Right</th>
          <td>8.740735</td>
          <td>4.189391</td>
          <td>2.086397</td>
          <td>0.036943</td>
          <td>0.529679</td>
          <td>16.951792</td>
          <td>hbo</td>
          <td>True</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 553-579

.. _tut-fnirs-group-relating:

Relating Responses to Brain Landmarks
-------------------------------------

.. sidebar:: fOLD Toolbox

   You should use the fOLD toolbox to pick your optode locations
   when designing your experiment.
   The tool is very intuitive and easy to use.
   Be sure to cite the authors if you use their tool or data:

   Morais, Guilherme Augusto Zimeo, Joana Bisol Balardin, and João Ricardo Sato. "fNIRS optodes’ location decider (fOLD): a toolbox for probe arrangement guided by brain regions-of-interest." Scientific reports 8.1 (2018): 1-11.

It can be useful to understand what brain structures
the measured response may have resulted from. Here we illustrate
how to report the brain structures/landmarks that the source
detector pair with the largest response was sensitive to.

First we determine the channel with the largest response.

Next, we query the fOLD dataset to determine the
brain landmarks that this channel is most sensitive to.
MNE-NIRS does not distribute the fOLD toolbox or the data
that they provide. See the Notes section of
:func:`mne_nirs.io.fold_channel_specificity` for more information.

.. GENERATED FROM PYTHON SOURCE LINES 579-584

.. code-block:: default


    largest_response_channel = ch_model_df.loc[ch_model_df['Coef.'].idxmax()]
    largest_response_channel






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    Coef.          19.332204
    Std.Err.        4.189391
    z               4.614561
    P>|z|           0.000004
    [0.025         11.121148
    0.975]          27.54326
    Chroma               hbo
    Significant         True
    Name: (S5_D7 hbo, Tapping_Left), dtype: object



.. GENERATED FROM PYTHON SOURCE LINES 585-590

Next we use information from the fOLD toolbox to report the
channel specificity to different brain regions.
For licensing reasons, these files are not distributed with MNE-NIRS.
To set up your system to use the fOLD functions, see the Notes section of
:func:`mne_nirs.io.fold_channel_specificity`.

.. GENERATED FROM PYTHON SOURCE LINES 591-596

.. code-block:: default


    raw_channel = raw_haemo.copy().pick(largest_response_channel.name[0])
    fold_channel_specificity(raw_channel)[0]







.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>Channel</th>
          <th>Source</th>
          <th>Detector</th>
          <th>Distance (mm)</th>
          <th>brainSens</th>
          <th>X (mm)</th>
          <th>Y (mm)</th>
          <th>Z (mm)</th>
          <th>Landmark</th>
          <th>Specificity</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>100.0</td>
          <td>C4</td>
          <td>C2</td>
          <td>40.0</td>
          <td>7.944795</td>
          <td>42.0</td>
          <td>-21.0</td>
          <td>62.0</td>
          <td>R Precentral Gyrus</td>
          <td>48.935362</td>
        </tr>
        <tr>
          <th>1</th>
          <td>100.0</td>
          <td>C4</td>
          <td>C2</td>
          <td>40.0</td>
          <td>7.944795</td>
          <td>42.0</td>
          <td>-21.0</td>
          <td>62.0</td>
          <td>R Postcentral Gyrus</td>
          <td>40.399862</td>
        </tr>
        <tr>
          <th>2</th>
          <td>100.0</td>
          <td>C4</td>
          <td>C2</td>
          <td>40.0</td>
          <td>7.944795</td>
          <td>42.0</td>
          <td>-21.0</td>
          <td>62.0</td>
          <td>R Inferior Parietal Lobule</td>
          <td>2.960631</td>
        </tr>
        <tr>
          <th>3</th>
          <td>100.0</td>
          <td>C4</td>
          <td>C2</td>
          <td>40.0</td>
          <td>7.944795</td>
          <td>42.0</td>
          <td>-21.0</td>
          <td>62.0</td>
          <td>R Superior Frontal Gyrus</td>
          <td>2.944289</td>
        </tr>
        <tr>
          <th>4</th>
          <td>100.0</td>
          <td>C4</td>
          <td>C2</td>
          <td>40.0</td>
          <td>7.944795</td>
          <td>42.0</td>
          <td>-21.0</td>
          <td>62.0</td>
          <td>R SupraMarginal Gyrus</td>
          <td>2.083585</td>
        </tr>
        <tr>
          <th>5</th>
          <td>100.0</td>
          <td>C4</td>
          <td>C2</td>
          <td>40.0</td>
          <td>7.944795</td>
          <td>42.0</td>
          <td>-21.0</td>
          <td>62.0</td>
          <td>R Middle Frontal Gyrus</td>
          <td>1.981430</td>
        </tr>
      </tbody>
    </table>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 597-601

We observe that the channel with the largest response to tapping
had the greatest specificity to the Precentral Gyrus, which is
the site of the primary motor cortex. This is consistent
with the expectation for a finger tapping task.

.. GENERATED FROM PYTHON SOURCE LINES 605-612

Conclusion
----------

This example has demonstrated how to perform a group level analysis
using a GLM approach.
We observed the responses were evoked primarily contralateral to the
hand of tapping and most likely originate from the primary motor cortex.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 0 minutes  55.058 seconds)

**Estimated memory usage:**  428 MB


.. _sphx_glr_download_auto_examples_general_plot_12_group_glm.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example


  .. container:: binder-badge

    .. image:: images/binder_badge_logo.svg
      :target: https://mybinder.org/v2/gh/mne-tools/mne-nirs/gh-pages?filepath=notebooks/auto_examples/general/plot_12_group_glm.ipynb
      :alt: Launch binder
      :width: 150 px


  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: plot_12_group_glm.py <plot_12_group_glm.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: plot_12_group_glm.ipynb <plot_12_group_glm.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
